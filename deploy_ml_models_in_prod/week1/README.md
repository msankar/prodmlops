# Model Serving: Introduction

*  Identify and contrast the challenges for serving inference requests
*  Compare cost, latency and throughput metrics to optimize serving inference requests
*  Judge the hardware resources and requirements for your serving models so that your system is reliable and can scale based on demand
*  Install and Use TensorFlow Serving to serve inference requests on a simple image classification model

## Tutorial Example
* [Build, train, and deploy an XGBoost model on Cloud AI Platform](https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/blob/main/course4/week1-ungraded-labs/C4_W1_Optional_Lab_1_XGBoost_CAIP/C4_W1_Optional_Lab_1.md) 
