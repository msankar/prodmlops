# Model Serving: Introduction

*  Identify and contrast the challenges for serving inference requests
*  Compare cost, latency and throughput metrics to optimize serving inference requests
*  Judge the hardware resources and requirements for your serving models so that your system is reliable and can scale based on demand
*  Install and Use TensorFlow Serving to serve inference requests on a simple image classification model