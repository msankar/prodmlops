# Deploying Machine Learning Models in Production

## 1 - Model Serving: Introduction
* Identify and contrast the challenges for serving inference requests
* Compare cost, latency and throughput metrics to optimize serving inference requests
* Judge the hardware resources and requirements for your serving models so that your system is reliable and can scale based on demand
* Install and Use TensorFlow Serving to serve inference requests on a simple image classification model

## 2 - Model Serving: Patterns and Infrastructure
* Serve models and deliver inference results by building scalable and reliable infrastructure.
* Contrast the use case for batch and realtime inference and how to optimize performance and hardware usage in each case
* Implement techniques to run inference on both edge devices and applications running in a web browser
* Outline and structure your data preprocessing pipeline to match your inference requirements
* Distinguish the performance and resource requirements for static and stream based batch inference

## 3 - Model Management and Delivery
* Coordinate model management, tracking and delivery by managing model versions, lineage, and registries
* Implement ML processes, pipelines, and workflow automation that adhere to modern MLOps practices
* Carry out continuous integration and delivery for model deployment, and maintain and monitor a continuously operating production system

## 4 - Model Monitoring and Logging
* Manage, monitor and audit your projects during their entire lifecycle
* Establish procedures to detect model decay and prevent reduced accuracy
* Implement modern practices to trace your ML system including monitoring and logging
* Integrate mechanisms to your production systems to ensure that they comply with modern responsible AI practices and legal regulations
* Determine when is appropriate and how to apply techniques to protect users privacy and respect their right to be forgotten
