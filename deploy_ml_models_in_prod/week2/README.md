# Model Serving: Introduction

* Serve models and deliver inference results by building scalable and reliable infrastructure.
* Contrast the use case for batch and realtime inference and how to optimize performance and hardware usage in each case
* Implement techniques to run inference on both edge devices and applications running in a web browser
* Outline and structure your data preprocessing pipeline to match your inference requirements
* Distinguish the performance and resource requirements for static and stream based batch inference

### Documentation on Model Servers

* [TensorFlow Serving](https://www.tensorflow.org/tfx/serving/architecture)

* [TorchServe](https://github.com/pytorch/serve)

* [KubeFlow Serving](https://www.kubeflow.org/docs/components/serving/)

* [NVIDIA Triton](https://developer.nvidia.com/nvidia-triton-inference-server)